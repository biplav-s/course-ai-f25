{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e0308b6-5fa7-4d4f-b791-153848dfbf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Based on code at:\n",
    "#  - https://docs.pytorch.org/tutorials/beginner/pytorch_with_examples.html\n",
    "#  - Cleaned up and hardcoding made as variables for clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c04f59-43af-4337-b904-65860eacf80d",
   "metadata": {},
   "source": [
    "Basic Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79fe8240-3158-4a82-b848-4d0384661aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "Num_data_samples = 2000\n",
    "# Create random input and output data\n",
    "x = np.linspace(-math.pi, math.pi, Num_data_samples)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6655d4f7-3f62-43da-9271-b0d2fd8d3279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1617.7257491795092\n",
      "199 1074.4080122492314\n",
      "299 714.6452743319054\n",
      "399 476.3995214523051\n",
      "499 318.6099773697166\n",
      "599 214.09505920277644\n",
      "699 144.85961435186064\n",
      "799 98.98925392484904\n",
      "899 68.59490721602583\n",
      "999 48.45236802689706\n",
      "1099 35.1017816162582\n",
      "1199 26.251534353191595\n",
      "1299 20.383618562434613\n",
      "1399 16.492356512505026\n",
      "1499 13.911402725324804\n",
      "1599 12.199187258433536\n",
      "1699 11.063050443184833\n",
      "1799 10.30899586327683\n",
      "1899 9.808407110819058\n",
      "1999 9.475998622466731\n",
      "2099 9.255206926554628\n",
      "2199 9.10851026551354\n",
      "2299 9.0110130530381\n",
      "2399 8.946193411246012\n",
      "Result: y = -0.0038022392025033828 + 0.8462655372239958 x + 0.0006559495319652625 x^2 + -0.09184036800323231 x^3\n"
     ]
    }
   ],
   "source": [
    "# Initialize to a low value\n",
    "learning_rate = 1e-6\n",
    "Num_rounds = 2400\n",
    "\n",
    "for t in range(Num_rounds):\n",
    "    # Forward pass: compute predicted y\n",
    "    # y = a + b x + c x^2 + d x^3\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db972b6a-b152-4de4-b439-743b646c5d9a",
   "metadata": {},
   "source": [
    "Using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f13532a-abdd-4d57-8253-5c0f911c4542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/biplavs/opt/anaconda3/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /Users/biplavs/opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/biplavs/opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Users/biplavs/opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /Users/biplavs/opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/biplavs/opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/biplavs/opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/biplavs/opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/biplavs/opt/anaconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Uncomment as needed\n",
    "# !pip install -U  torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d49efa1-1ebf-4786-8640-6034591f55e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# We want to be able to train our model on an `accelerator <https://pytorch.org/docs/stable/torch.html#accelerators>`__\n",
    "# such as CUDA, MPS, MTIA, or XPU. If the current accelerator is available, we will use it. Otherwise, we use the CPU.\n",
    "\n",
    "dtype = torch.float\n",
    "device = \"cpu\"\n",
    "#device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "torch.set_default_device(device)\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "Num_data_samples = 2000\n",
    "x = torch.linspace(-math.pi, math.pi, Num_data_samples, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For a third order polynomial, we need\n",
    "# 4 weights: y = a + b x + c x^2 + d x^3\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), dtype=dtype, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba73e343-5a90-4512-8d0c-c93735e92493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 788.2459106445312\n",
      "199 529.4957885742188\n",
      "299 356.85321044921875\n",
      "399 241.600830078125\n",
      "499 164.61688232421875\n",
      "599 113.16415405273438\n",
      "699 78.75384521484375\n",
      "799 55.726043701171875\n",
      "899 40.30509948730469\n",
      "999 29.970699310302734\n",
      "1099 23.039966583251953\n",
      "1199 18.388355255126953\n",
      "1299 15.26382827758789\n",
      "1399 13.163288116455078\n",
      "1499 11.749972343444824\n",
      "1599 10.798200607299805\n",
      "1699 10.156644821166992\n",
      "1799 9.723793029785156\n",
      "1899 9.431455612182617\n",
      "1999 9.233831405639648\n",
      "2099 9.10009765625\n",
      "2199 9.009488105773926\n",
      "2299 8.948043823242188\n",
      "2399 8.906332969665527\n",
      "Result: y = -0.006694426294416189 + 0.8499247431755066 x + 0.0011548977345228195 x^2 + -0.09236085414886475 x^3\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-6\n",
    "Num_rounds = 2400\n",
    "\n",
    "for t in range(Num_rounds):\n",
    "    # Forward pass: compute predicted y using operations on Tensors.\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b85cd0-e946-4c83-a64c-b5179f1418b2",
   "metadata": {},
   "source": [
    "Illustrating PyTorch Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeec86f2-0576-4d36-b82f-38a8460fdf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "Num_data_samples = 2000\n",
    "# Create Tensors to hold input and outputs.\n",
    "x = torch.linspace(-math.pi, math.pi, Num_data_samples)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Prepare the input tensor (x, x^2, x^3).\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2236295f-2a8d-41f1-ae07-b68ebc0ba929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 32466.4921875\n",
      "199 15057.2236328125\n",
      "299 5981.9755859375\n",
      "399 1732.780517578125\n",
      "499 290.064453125\n",
      "599 27.054338455200195\n",
      "699 10.477029800415039\n",
      "799 9.66016960144043\n",
      "899 9.270905494689941\n",
      "999 9.075874328613281\n",
      "1099 8.944184303283691\n",
      "1199 8.85744857788086\n",
      "1299 8.823195457458496\n",
      "1399 8.817428588867188\n",
      "1499 8.817171096801758\n",
      "1599 8.855120658874512\n",
      "1699 8.817290306091309\n",
      "1799 8.965662002563477\n",
      "1899 8.997116088867188\n",
      "1999 8.890856742858887\n",
      "2099 8.873276710510254\n",
      "2199 8.90768051147461\n",
      "2299 8.94318962097168\n",
      "2399 8.923628807067871\n",
      "Result: y = -0.0005040726391598582 + 0.8572428226470947 x + -0.0005040737451054156 x^2 + -0.09282831102609634 x^3\n"
     ]
    }
   ],
   "source": [
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use RMSprop; the optim package contains many other\n",
    "# optimization algorithms. The first argument to the RMSprop constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "\n",
    "learning_rate = 1e-3\n",
    "#learning_rate = 1e-6       // seems to give delayed convergence in contrast to others \n",
    "Num_rounds = 2400\n",
    "\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "for t in range(Num_rounds):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable\n",
    "    # weights of the model). This is because by default, gradients are\n",
    "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "linear_layer = model[0]\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d26d819-9dac-4d60-84bb-ca3d8c2134e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
